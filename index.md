---
layout: default
title: Home
---
<h1>mlbench: Distributed Machine Learning Benchmark</h1>

<a href="https://travis-ci.com/mlbench/mlbench-dashboard"><img src="https://travis-ci.com/mlbench/mlbench-dashboard.svg?branch=develop"></a>
<a href="https://travis-ci.com/mlbench/mlbench-core"><img src="https://travis-ci.com/mlbench/mlbench-core.svg?branch=develop"></a>
<a href="https://travis-ci.com/mlbench/mlbench-benchmarks"><img src="https://travis-ci.com/mlbench/mlbench-benchmarks.svg?branch=develop"></a>
<a href="https://mlbench.readthedocs.io/en/latest/?badge=latest"><img src="https://readthedocs.org/projects/mlbench/badge/?version=latest" alt="Documentation Status"></a>





<p>
MLBench is a framework for distributed machine learning. Its purpose is to improve transparency, reproducibility, robustness, and to provide fair performance measures as well as reference implementations, helping adoption of distributed machine learning methods both in industry and in the academic community.
</p>

<p>
MLBench is public, open source and vendor independent, and has two main goals:

    <ul>
        <li>to be an easy-to-use and fair benchmarking suite for algorithms as well as for systems (software frameworks and hardware).</li>
        <li>to provide re-usable and reliable reference implementations of distributed ML algorithms.</li>
    </ul>

</p>

<p>
 For more details on the benchmarking tasks, see <a href="https://mlbench.readthedocs.io/en/latest/benchmark-tasks.html"> Benchmark Tasks</a> and <a href="https://mlbench.readthedocs.io/en/latest/benchmark-tasks.html#benchmark-task-results">Benchmark Results</a>
</p>

<p>
Check out our <a href="https://mlbench.github.io/blog/">blog</a>!
</p>

<p>
<h2>Resources and Community:</h2>

    <ul>
        <li>Github: <a href="https://github.com/mlbench/">github.com/mlbench</a></li>
        <li>Documentation: <a href="https://mlbench.readthedocs.io">mlbench.readthedocs.io</a></li>
        <li>Mailing list: <a href="https://groups.google.com/d/forum/mlbench">groups.google.com/d/forum/mlbench</a></li>
        <li>
            Slack channel:
            <a href="https://join.slack.com/t/mlbench/shared_invite/enQtNTMzMjkwNDA4MTAyLWM2NTQ4OGNjMjFmZDdhMGQ2YzZkYmUyNzMxN2QxZjc2MmQyNjNiNjM3YWFlYzAwZjQ0Yzc0MTc3YWY5Zjg1ZjU">join.slack.com/t/mlbench/shared_invite/enQtNTMzMjkwNDA4MTAyLWM2NTQ4OGNjMjFmZDdhMGQ2YzZkYmUyNzMxN2QxZjc2MmQyNjNiNjM3YWFlYzAwZjQ0Yzc0MTc3YWY5Zjg1ZjU
            </a>
        </li>
    </ul>

    <h2>Features</h2>

    <ul>
        <li> For reproducibility and simplicity, we currently focus on standard <strong>supervised ML</strong>, including standard deep learning tasks as well as classic linear ML models.</li>
        <li> We provide <strong>reference implementations</strong> for each algorithm and task, to make it easy to port to a new framework.</li>
        <li> Our goal is to benchmark all/most currently relevant <strong>distributed execution frameworks</strong>. We welcome contributions of new frameworks in the benchmark suite.</li>
        <li> We provide <strong>precisely defined tasks</strong> and datasets to have a fair and precise comparison of all algorithms, frameworks and hardware.</li>
        <li> Independently of all solver implementations, we provide universal <strong>evaluation code</strong> allowing to compare the result metrics of different solvers and frameworks.</li>
        <li> Our benchmark code is easy to run on <strong>public clouds</strong>.</li>
    </ul>

    <h2>Sponsors</h2>

    <ul style="list-style-type:none;">
        <li><img src="{{ site.baseurl }}public/images/Logo_EPFL.png" alt="EPFL" style="max-width:100px;border-radius:0px;"/></li>
        <li><img src="{{ site.baseurl }}public/images/pwc_logo.png" alt="PwC" style="max-width:110px;"/></li>
        <li><img src="{{ site.baseurl }}public/images/google.jpg" alt="Google" style="max-width:100px"/></li>
    </ul>

</p>

